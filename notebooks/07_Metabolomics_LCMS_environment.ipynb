{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cdf589c",
   "metadata": {},
   "source": [
    "# Metabolomica LC–MS: feature table, QC, normalizzazione e analisi\n",
    "\n",
    "Obiettivi del notebook:\n",
    "- Simulare un piccolo dataset di **metabolomica LC–MS untargeted** (feature m/z–RT × campioni).\n",
    "- Eseguire passi base di **QC** (TIC, distribuzioni di intensità).\n",
    "- Applicare una **normalizzazione** semplice.\n",
    "- Eseguire una **PCA** e una **analisi differenziale** (t-test + volcano plot).\n",
    "\n",
    "_Tutti i dati sono simulati per fini didattici, ispirati a un tipico peak table LC–MS._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5061c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a859b",
   "metadata": {},
   "source": [
    "## 1. Simulazione di una tabella di feature LC–MS\n",
    "\n",
    "Impostiamo:\n",
    "- 250 feature (picchi) descritte da m/z, RT (minuti) e intensità.\n",
    "- 10 campioni: 5 controlli (CTRL) e 5 esposti (EXP) – ad es. organismi in acqua pulita vs inquinata.\n",
    "- La maggior parte delle feature non mostra differenze sistematiche.\n",
    "- Un piccolo set di feature \"biomarker\" mostra intensità maggiori nel gruppo esposto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2894cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 250\n",
    "n_ctrl = 5\n",
    "n_exp = 5\n",
    "n_samples = n_ctrl + n_exp\n",
    "\n",
    "sample_ids = [f\"CTRL_{i+1}\" for i in range(n_ctrl)] + [f\"EXP_{i+1}\" for i in range(n_exp)]\n",
    "conditions = pd.Series([\n",
    "    \"CTRL\" for _ in range(n_ctrl)\n",
    "] + [\n",
    "    \"EXP\" for _ in range(n_exp)\n",
    "], index=sample_ids, name=\"condition\")\n",
    "\n",
    "# m/z e RT fittizi\n",
    "mz_values = np.random.uniform(80, 1000, size=n_features)\n",
    "rt_values = np.random.uniform(0.5, 20.0, size=n_features)  # minuti\n",
    "\n",
    "feature_ids = [f\"FT_{i+1:04d}\" for i in range(n_features)]\n",
    "\n",
    "# Intensità base log10 ~ N(5, 0.5)\n",
    "log10_intens_base = np.random.normal(loc=5.0, scale=0.5, size=(n_features, n_samples))\n",
    "intensities = 10 ** log10_intens_base\n",
    "\n",
    "# Selezioniamo 15 feature come \"differenziali\" (più alte negli esposti)\n",
    "n_de = 15\n",
    "de_idx = np.random.choice(np.arange(n_features), size=n_de, replace=False)\n",
    "for idx in de_idx:\n",
    "    # aumento di ~0.6 log10 (≈ x4 in intensità) nel gruppo EXP\n",
    "    intensities[idx, conditions.values == \"EXP\"] *= (10 ** 0.6)\n",
    "\n",
    "# Aggiungiamo un po' di missing values (es. feature a bassa intensità)\n",
    "scaled_mean = intensities.mean(axis=1) / intensities.mean()\n",
    "prob_missing = np.interp(scaled_mean, (scaled_mean.min(), scaled_mean.max()), (0.4, 0.02))\n",
    "mask_missing = np.zeros_like(intensities, dtype=bool)\n",
    "for i in range(n_features):\n",
    "    for j in range(n_samples):\n",
    "        if np.random.rand() < prob_missing[i]:\n",
    "            mask_missing[i, j] = True\n",
    "intensities_masked = intensities.copy()\n",
    "intensities_masked[mask_missing] = np.nan\n",
    "\n",
    "intensity_cols = [f\"INT_{sid}\" for sid in sample_ids]\n",
    "data = pd.DataFrame(intensities_masked, columns=intensity_cols)\n",
    "data.insert(0, \"RT_min\", rt_values)\n",
    "data.insert(0, \"mz\", mz_values)\n",
    "data.insert(0, \"FeatureID\", feature_ids)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbbe285",
   "metadata": {},
   "source": [
    "## 2. QC: Total Ion Current (TIC) per campione\n",
    "\n",
    "Calcoliamo per ciascun campione la somma delle intensità (TIC) come misura grossolana di carico totale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20ae273",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = data[intensity_cols].sum(axis=0, skipna=True)\n",
    "tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3577644",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(intensity_cols)), tic.values)\n",
    "plt.xticks(range(len(intensity_cols)), intensity_cols, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"TIC (somma intensità)\")\n",
    "plt.title(\"Total Ion Current per campione\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd69997e",
   "metadata": {},
   "source": [
    "## 3. Distribuzioni di intensità (log10) per campione\n",
    "\n",
    "Convertiamo le intensità in log10 e confrontiamo le distribuzioni tra campioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24889eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log10_data = np.log10(data[intensity_cols])\n",
    "plt.figure(figsize=(6,4))\n",
    "for sid in sample_ids:\n",
    "    col = f\"INT_{sid}\"\n",
    "    plt.hist(log10_data[col].dropna(), bins=30, alpha=0.5, label=sid)\n",
    "plt.xlabel(\"log10 intensità\")\n",
    "plt.ylabel(\"Conteggio\")\n",
    "plt.title(\"Distribuzioni grezze delle intensità (log10)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861cb687",
   "metadata": {},
   "source": [
    "In dataset reali, differenze globali tra campioni richiedono **normalizzazione**.\n",
    "\n",
    "## 4. Normalizzazione per TIC\n",
    "\n",
    "Applichiamo una normalizzazione semplice: dividiamo le intensità di ciascun campione per il suo TIC e riportiamo in log10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62f6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_intens = data[intensity_cols].copy()\n",
    "for col in intensity_cols:\n",
    "    norm_intens[col] = norm_intens[col] / tic[col]\n",
    "\n",
    "norm_log10 = np.log10(norm_intens)\n",
    "plt.figure(figsize=(6,4))\n",
    "for sid in sample_ids:\n",
    "    col = f\"INT_{sid}\"\n",
    "    plt.hist(norm_log10[col].dropna(), bins=30, alpha=0.5, label=sid)\n",
    "plt.xlabel(\"log10 intensità normalizzate (TIC)\")\n",
    "plt.ylabel(\"Conteggio\")\n",
    "plt.title(\"Distribuzioni dopo normalizzazione TIC\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ed27c5",
   "metadata": {},
   "source": [
    "Aggiorniamo il nostro DataFrame con le intensità normalizzate (log10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d473fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data = data.copy()\n",
    "for col in intensity_cols:\n",
    "    norm_data[col] = norm_log10[col]\n",
    "norm_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d127dae",
   "metadata": {},
   "source": [
    "## 5. Filtraggio di feature con troppi missing\n",
    "\n",
    "Per un'analisi statistica semplice, richiediamo che una feature sia presente in\n",
    "- almeno 4/5 campioni in ciascun gruppo (CTRL ed EXP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ffccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_cols = [f\"INT_CTRL_{i+1}\" for i in range(n_ctrl)]\n",
    "exp_cols = [f\"INT_EXP_{i+1}\" for i in range(n_exp)]\n",
    "\n",
    "mask_ctrl = norm_data[ctrl_cols].notna().sum(axis=1) >= 4\n",
    "mask_exp = norm_data[exp_cols].notna().sum(axis=1) >= 4\n",
    "keep = mask_ctrl & mask_exp\n",
    "filtered = norm_data.loc[keep].reset_index(drop=True)\n",
    "print(f\"Feature totali: {len(norm_data)}\")\n",
    "print(f\"Feature dopo filtro missing: {len(filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6581d",
   "metadata": {},
   "source": [
    "## 6. PCA sui campioni\n",
    "\n",
    "Applichiamo una PCA \"fatta a mano\" (via SVD) sulle intensità normalizzate, per vedere se i campioni CTRL ed EXP si separano.\n",
    "\n",
    "1. Usiamo solo le feature filtrate.\n",
    "2. Centriamo per feature (sottraiamo la media).\n",
    "3. Calcoliamo la SVD e prendiamo le prime due componenti principali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c239660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice feature × campioni (usiamo .T per avere campioni × feature)\n",
    "X = filtered[intensity_cols].values  # righe = feature, colonne = campioni\n",
    "X = X.T  # righe = campioni, colonne = feature\n",
    "\n",
    "# Sostituiamo eventuali NaN con la media della feature (colonna)\n",
    "col_means = np.nanmean(X, axis=0)\n",
    "inds = np.where(np.isnan(X))\n",
    "X[inds] = np.take(col_means, inds[1])\n",
    "\n",
    "# Centriamo per colonna (feature)\n",
    "X_centered = X - X.mean(axis=0, keepdims=True)\n",
    "\n",
    "# SVD\n",
    "U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "PCs = U[:, :2] * S[:2]  # punteggi delle prime due PC\n",
    "\n",
    "pca_df = pd.DataFrame(\n",
    "    PCs,\n",
    "    columns=[\"PC1\", \"PC2\"],\n",
    "    index=sample_ids,\n",
    ")\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5a6dd",
   "metadata": {},
   "source": [
    "### Plot PCA (PC1 vs PC2)\n",
    "\n",
    "I campioni dovrebbero mostrarsi in qualche modo separati tra CTRL ed EXP se le feature differenziali sono abbastanza marcate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "for sid in sample_ids:\n",
    "    cond = conditions[sid]\n",
    "    x = pca_df.loc[sid, \"PC1\"]\n",
    "    y = pca_df.loc[sid, \"PC2\"]\n",
    "    plt.scatter(x, y)\n",
    "    plt.text(x+0.1, y, sid, fontsize=8)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA dei campioni (LC–MS simulato)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4292c44",
   "metadata": {},
   "source": [
    "## 7. Analisi differenziale feature per feature\n",
    "\n",
    "Per ogni feature filtrata:\n",
    "- calcoliamo la media nei due gruppi.\n",
    "- log2 fold change = EXP − CTRL (usando le intensità log10 normalizzate, convertite a log2 per la leggibilità).\n",
    "- eseguiamo un t-test (CTRL vs EXP).\n",
    "\n",
    "_Nota_: lavoriamo su log10, ma convertiamo la differenza a scala log2 per interpretare facilmente le fold-change._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f229817",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "log2_factor = np.log2(10)  # conversione da log10 a log2\n",
    "for idx, row in filtered.iterrows():\n",
    "    vals_ctrl = row[ctrl_cols].values\n",
    "    vals_exp = row[exp_cols].values\n",
    "    mean_ctrl = np.nanmean(vals_ctrl)\n",
    "    mean_exp = np.nanmean(vals_exp)\n",
    "    # differenza in log10\n",
    "    diff_log10 = mean_exp - mean_ctrl\n",
    "    log2_fc = diff_log10 * log2_factor\n",
    "    tstat, pval = stats.ttest_ind(vals_exp, vals_ctrl, equal_var=False, nan_policy=\"omit\")\n",
    "    results.append({\n",
    "        \"FeatureID\": row[\"FeatureID\"],\n",
    "        \"mz\": row[\"mz\"],\n",
    "        \"RT_min\": row[\"RT_min\"],\n",
    "        \"mean_CTRL_log10\": mean_ctrl,\n",
    "        \"mean_EXP_log10\": mean_exp,\n",
    "        \"log2_FC_EXP_vs_CTRL\": log2_fc,\n",
    "        \"pval\": pval,\n",
    "    })\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65758fcf",
   "metadata": {},
   "source": [
    "Aggiungiamo −log₁₀(p-value) e costruiamo un volcano plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa36afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df[\"neg_log10_pval\"] = -np.log10(res_df[\"pval\"])\n",
    "res_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee21f7",
   "metadata": {},
   "source": [
    "## 8. Volcano plot\n",
    "\n",
    "Plot:\n",
    "- asse X: log₂ FC\n",
    "- asse Y: −log₁₀(p-value)\n",
    "\n",
    "Soglie indicative:\n",
    "- |log₂FC| ≥ 1 (≈ FC ≥ 2)\n",
    "- p-value ≤ 0.01 (≈ −log₁₀(p) ≥ 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4f5412",
   "metadata": {},
   "outputs": [],
   "source": [
    "log2fc = res_df[\"log2_FC_EXP_vs_CTRL\"]\n",
    "neglogp = res_df[\"neg_log10_pval\"]\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(log2fc, neglogp, alpha=0.5)\n",
    "plt.axvline(1, linestyle=\"--\")\n",
    "plt.axvline(-1, linestyle=\"--\")\n",
    "plt.axhline(2, linestyle=\"--\")\n",
    "plt.xlabel(\"log2 FC (EXP vs CTRL)\")\n",
    "plt.ylabel(\"-log10 p-value\")\n",
    "plt.title(\"Volcano plot (metabolomica LC–MS simulata)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c8313",
   "metadata": {},
   "source": [
    "## 9. Feature candidate come biomarcatori metabolici\n",
    "\n",
    "Applichiamo le soglie:\n",
    "- |log₂FC| ≥ 1\n",
    "- p-value ≤ 0.01\n",
    "\n",
    "Queste feature potrebbero corrispondere a metaboliti legati a stress ossidativo, energia, osmoprotezione, ecc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7563f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_thresh = 1.0\n",
    "p_thresh = 0.01\n",
    "hits = res_df[(res_df[\"log2_FC_EXP_vs_CTRL\"].abs() >= fc_thresh) & (res_df[\"pval\"] <= p_thresh)]\n",
    "print(f\"Numero di feature candidate: {len(hits)}\")\n",
    "hits.sort_values(\"pval\").head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abceeed8",
   "metadata": {},
   "source": [
    "In un dataset reale, queste feature verrebbero:\n",
    "- annottate contro database metabolici (HMDB, KEGG, METLIN, ...)\n",
    "- collegate a pathway (es. stress ossidativo, TCA, metabolismo degli amminoacidi).\n",
    "\n",
    "Qui abbiamo visto in pratica la pipeline concettuale della Lecture 6:\n",
    "1. Peak table LC–MS →\n",
    "2. QC (TIC, distribuzioni) →\n",
    "3. Normalizzazione →\n",
    "4. PCA →\n",
    "5. Analisi differenziale e selezione di metaboliti candidati legati all'esposizione ambientale."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
